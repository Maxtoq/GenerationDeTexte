A language model (LM) is a statistical model that assigns a probability to a sequence of words by generating
a probability distribution. This is useful in a variety of Natural Language Processing (NLP) tasks. For
example, speech recognizers profit from a probability assigned to the next word in a speech sequence to
be predicted. Similarly, Part-of-Speech (POS) taggers, syntactical parsers or word sense disambiguation
systems all include features which an LM can provide and would inform the systems of tags (generalizations
over words) and/or the word embeddings in the context of a word sequence. In Machine Translation systems,
an LM is used to tune the probability scores of outputs by the system in order to improve for the actual
grammaticality and fluency of a translation in the target language.
However, as there is a potentially infinite possible combination of words in natural language, it is difficult
to model generalizations in training or testing time that is still practical for real-time processing systems.
In most applications, language modeling has therefore been solved by using a statistical model of relative
frequency counts. The so-called n-gram models count uni-, bi- or tri-grams (sometimes also 4- and 5-grams),
i.e. a word in a sequence is predicted based on its preceding context (history) of 1 to 4 words.
This approach is accompanied by several problems: it explicitly matches words or strings and it is very
likely that quite a big amount of the n-grams are not seen during training even with very large corpora (i.e.
millions of sentences). This leads to data sparsity and possible over-fitting of the training data and to the
related problem that unseen word sequences cannot be assigned a probability during testing time. A variety
of smoothing techniques has been developed that currently are the state-of-the-art for n-gram language
modeling and that allow for correcting/generalizing the relative frequency counts of the seen n-grams.
However, what would actually be the goal of language modeling is to recognize that word sequences such
as he has seen the man walking in the park to be grammatically and semantically similar to she has seen
a woman running in the park. POS tagging or syntactical parsing would provide generalizations over such
sequences (you model sequences of tags instead of explicit words), but they themselves need costly manual
annotation for supervised learning and engineered linguistic (expert) features.
In the 1980s and early 90s Artificial Neural Networks (NN) were a popular learning technique that
was capable of learning such features automatically during training (not using any feature engineering or
manually annotated class labels). The idea was to mimic the learning procedure of the human brain with
connected neurons that are activated or not during a learning phase. NNs however lost their popularity due to
immense amounts of training time one has needed back then. With the advent of faster and parallel computer
architectures in recent years, they however regained success and have been further developed to multilayer,
recurrent and/or deep neural networks that perform at the state-of-the-art of other machine learning tasks
including language modeling, where the better generalizations over word sequences provided by an NN model
outperform the n-gram models.
This survey provides a close look at training and testing methods of NNs applied to language modeling.
We introduce into both fields, Section 2.1 for NNs and Section 2.2 for LMs. The main section of the survey,
Section 3, then subsumes several NN-LM techniques, an overview of the basic formalisms and architectures
of the first Neural Network Language Models (NN-LM) is given in Section 3.2. The following subsections
describe advanced methods to be used for large training data (Section 3.3.1), combinations with n-gram
modeling (Section 3.3.3) or related methods that have been investigated (Section 3.3.4). Section 3.4 looks
at state-of-the-art NN-LM methods before the survey is concluded in Section 4 by outlining remaining
challenges.
